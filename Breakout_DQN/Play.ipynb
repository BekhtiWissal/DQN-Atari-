{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "episodes, scores, frames, start_episode, steps_done = [], [], [], 0, 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i_episode in range(start_episode, start_episode+100):\n",
    "    frame, _= env.reset()\n",
    "    frame = resize_frame(frame)\n",
    "    # We start with 30 no-op actions\n",
    "    for i in range(no_op_action):\n",
    "        starting_frame = resize_frame(env.step(0)[0])\n",
    "    i=0\n",
    "    while i<3:\n",
    "        frames.append(starting_frame)\n",
    "        i+=1\n",
    "    state = get_state([frames[-3], frames[-2], frames[-1], frame])\n",
    "\n",
    "    for t in range(1000):\n",
    "        with torch.no_grad():\n",
    "            action = agent_explore(state)\n",
    "        frame, reward, terminated, truncated, _= env.step(action)\n",
    "        #rescaled, and gray-scale, version of the last four frames\n",
    "        frame = resize_frame(frame)\n",
    "        frames.append(frame)\n",
    "        reward = torch.tensor([reward], device=device)\n",
    "        done = terminated or truncated\n",
    "\n",
    "        if terminated:\n",
    "            next_state = None\n",
    "        else:\n",
    "            next_state = get_state([frames[-3], frames[-2], frames[-1], frame])\n",
    "\n",
    "        memory.push(state, action, next_state, reward)\n",
    "\n",
    "        state = next_state\n",
    "\n",
    "        if steps_done >= INITIAL_MEMORY and steps_done % UPDATE_FREQ == 0:\n",
    "            # The memory gets sampled to update the network every 4\n",
    "            # steps with minibatches of size 32\n",
    "            # Every 4 actions, a gradient descent step is performed and Q-values are updated\n",
    "            optimize_model()\n",
    "\n",
    "        if steps_done % TARGET_UPDATE == 0:\n",
    "            update_target_network(policy_net, target_net, TAU)\n",
    "\n",
    "        if done:\n",
    "            break\n",
    "\n",
    "    scheduler.step()\n",
    "    done = False\n",
    "    score, i = 0, 0\n",
    "    frame, _= env.reset()\n",
    "    frame = resize_frame(frame)\n",
    "    starting_frame = resize_frame(env.step(0)[0])\n",
    "    while i in (0,3):\n",
    "        frames.append(starting_frame)\n",
    "        i+=1\n",
    "    state=get_state([frames[-3], frames[-2], frames[-1], frame])\n",
    "    for t in range(1000):\n",
    "        state = state.unsqueeze(0)\n",
    "        action = policy_net(state).max(1)[1].view(1, 1)\n",
    "        frame, reward, terminated, truncated, _= env.step(action.item())\n",
    "        print(\"actions throughout steps and episodes\", action.item(), t, i_episode)\n",
    "        frame = resize_frame(frame)\n",
    "        frames.append(frame)\n",
    "        score += reward\n",
    "        done = terminated or truncated\n",
    "\n",
    "        if not done:\n",
    "            next_state = get_state([frames[-3], frames[-2], frames[-1], frame])\n",
    "        else:\n",
    "            next_state=None\n",
    "        state = next_state\n",
    "        if done:\n",
    "            break\n",
    "\n",
    "    plot_durations()\n",
    "    episodes.append(i_episode)\n",
    "    scores.append(score)\n",
    "\n",
    "    if (i_episode+1) % 5000 == 0:\n",
    "        #We store only the minimum number of frames to train the next state\n",
    "        frames = frames[-4:-1]\n",
    "        print(f\"Saving weights at Episode {i_episode+1} ...\")\n",
    "        torch.save(model.state_dict(), model_file_path)\n",
    "        torch.save(data, data_file_path)\n",
    "\n",
    "print('Complete')\n",
    "plot_durations(show_result=True)\n",
    "plt.ioff()\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
