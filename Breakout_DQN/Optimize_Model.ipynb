{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def state_batch_right_dim1(state_batch):\n",
    "    states_batch, state_action_values, i = [], [], 0\n",
    "    while i<len(state_batch):\n",
    "        state = state_batch[i:i+4]\n",
    "        state = torch.tensor(state, dtype=torch.float32, device=device).unsqueeze(0)\n",
    "        states_batch.append(state)\n",
    "        state_action_value=policy_net(state)\n",
    "        state_action_values.append(state_action_value)\n",
    "        i+=4\n",
    "    return torch.cat(state_action_values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def state_batch_right_dim11(non_final_next_states):\n",
    "    non_final_next_statess, i = [], 0\n",
    "    while i<len(non_final_next_states):\n",
    "        state_None = non_final_next_states[i:i+4]\n",
    "        state_None = torch.tensor(state_None, dtype=torch.float32, device=device).unsqueeze(0)\n",
    "        non_final_next_state = target_net(state_None).max(1)[0].detach()\n",
    "        non_final_next_statess.append(non_final_next_state)\n",
    "        i+=4\n",
    "    return torch.cat(non_final_next_statess)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def optimize_model():\n",
    "    #unzip batch transitions // calculate q values/ next q values/ and target q values\n",
    "    if len(memory) < BATCH_SIZE:\n",
    "        return\n",
    "    transitions = memory.sample(BATCH_SIZE)\n",
    "\n",
    "    batch = Transition(*zip(*transitions))\n",
    "    state_batch = torch.cat(batch.state)\n",
    "    action_batch = torch.cat(batch.action)\n",
    "    reward_batch = torch.cat(batch.reward)\n",
    "    #account for the next zero q values (if life is terminated)\n",
    "\n",
    "    non_final_mask = torch.tensor(tuple(map(lambda s: s is not None,\n",
    "                                        batch.next_state)), device=device, dtype=torch.bool)\n",
    "    non_final_next_states = torch.cat([s for s in batch.next_state\n",
    "                                       if s is not None])\n",
    "    state_action_values = state_batch_right_dim1(state_batch).gather(1, action_batch)  #q_values #(q_values(shape==32), 1)\n",
    "    print(\"state_action_values\", state_action_values)\n",
    "    epsilon = 0.05\n",
    "    next_state_values = torch.zeros(32, device=device) #32==batch_size\n",
    "\n",
    "    with torch.no_grad():\n",
    "        if non_final_mask.any():\n",
    "            next_state_values[non_final_mask] = state_batch_right_dim11(non_final_next_states)\n",
    "\n",
    "    expected_state_action_values = (next_state_values * GAMMA) + reward_batch\n",
    "\n",
    "    # Compute Huber loss\n",
    "    criterion = nn.SmoothL1Loss()\n",
    "    loss = criterion(state_action_values, expected_state_action_values.unsqueeze(1))\n",
    "\n",
    "    # Optimize the model\n",
    "    optimizer.zero_grad()\n",
    "    loss.backward()\n",
    "    # In-place gradient clipping\n",
    "    #torch.nn.utils.clip_grad_value_(policy_net.parameters(), 100)\n",
    "    optimizer.step()"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
